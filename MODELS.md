# Model Leaderboard

Per-repo performance data for **netbird-app** from pattern-tracker and response-scoring databases.
Auto-generated by `generate-models-md.sh` â€” do not edit manually.

**Last updated**: 2026-02-28T01:31:34Z

- **Pattern data points**: 0
- **Scored responses**: 18
- **Scope**: netbird-app (repo-specific)

## Available Models

| Model | Provider | Tier | Context | Input/1M | Output/1M |
|-------|----------|------|---------|----------|-----------|
| claude-opus-4-6 | Anthropic | opus | 200K | $5.00 | $25.00 |
| o3 | OpenAI | opus | 200K | $10.00 | $40.00 |
| claude-sonnet-4-6 | Anthropic | sonnet | 200K | $3.00 | $15.00 |
| gemini-2.5-pro | Google | sonnet | 1M | $1.25 | $10.00 |
| gpt-4.1 | OpenAI | sonnet | 1M | $2.00 | $8.00 |
| gpt-4o | OpenAI | sonnet | 128K | $2.50 | $10.00 |
| o4-mini | OpenAI | sonnet | 200K | $1.10 | $4.40 |
| claude-haiku-4-5 | Anthropic | haiku | 200K | $1.00 | $5.00 |
| deepseek-r1 | DeepSeek | haiku | 131K | $0.55 | $2.19 |
| deepseek-v3 | DeepSeek | haiku | 131K | $0.27 | $1.10 |
| gemini-2.0-flash | Google | haiku | 1M | $0.10 | $0.40 |
| gemini-2.5-flash | Google | haiku | 1M | $0.15 | $0.60 |
| llama-4-maverick | Meta | haiku | 1M | $0.20 | $0.60 |
| llama-4-scout | Meta | haiku | 512K | $0.15 | $0.40 |
| gpt-4.1-mini | OpenAI | haiku | 1M | $0.40 | $1.60 |
| gpt-4.1-nano | OpenAI | haiku | 1M | $0.10 | $0.40 |
| gpt-4o-mini | OpenAI | haiku | 128K | $0.15 | $0.60 |

## Routing Tiers

Active model assignments for each dispatch tier:

| Tier | Primary Model | Relative Cost |
|------|---------------|---------------|
| haiku | claude-haiku-4-5 | ~0.33x |
| flash | gemini-2.5-flash-preview-05-20 | ~0.20x |
| sonnet | claude-sonnet-4-6 | 1x (baseline) |
| pro | gemini-2.5-pro-preview-06-05 | ~1.5x |
| opus | claude-opus-4-6 | ~1.7x |

## Performance Leaderboard

Success rates from autonomous task execution (pattern-tracker data):

No pattern data available yet. Run tasks to build the leaderboard.

## Contest Results

Quality evaluations from model comparison sessions (response-scoring data):

### Quality Scores

Weighted average across all evaluated responses (correctness 30%, completeness 25%, code quality 25%, clarity 20%):

| Model | Responses | Avg Score | Avg Time (s) |
|-------|-----------|-----------|--------------|
| claude-opus-4 | 6 | 4.56/5.0 | 0.0 |
| claude-sonnet-4 | 6 | 4.28/5.0 | 0.0 |
| gemini-2.5-pro | 6 | 4.11/5.0 | 0.0 |

---

*Generated by [aidevops](https://github.com/marcusquinn/aidevops) t1012, t1133*
